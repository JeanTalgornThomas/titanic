{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.sql.functions import when\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf()\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"test\").config(conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---+------+\n",
      "|PassengerId|Survived|Pclass|Age|Gender|\n",
      "+-----------+--------+------+---+------+\n",
      "|          1|       0|     3| 22|     1|\n",
      "|          2|       1|     1| 38|     2|\n",
      "|          3|       1|     3| 26|     2|\n",
      "|          4|       1|     1| 35|     2|\n",
      "|          5|       0|     3| 35|     1|\n",
      "|          7|       0|     1| 54|     1|\n",
      "|          8|       0|     3|  2|     1|\n",
      "|          9|       1|     3| 27|     2|\n",
      "|         10|       1|     2| 14|     2|\n",
      "|         11|       1|     3|  4|     2|\n",
      "+-----------+--------+------+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing the train\n",
    "train = spark.read.option(\"header\", \"true\").csv(r\"data\\train.csv\")\n",
    "# filtre et supprime les donn√©es \"NA\"\n",
    "train = train.filter(train.Age != \"NA\")\n",
    "train = train.withColumn(\"Gender\", when(train.Sex == \"male\",\"1\").when(train.Sex == \"female\",\"2\"))\n",
    "# Supprime les colonnes inutiles\n",
    "cols = ('SibSp', 'Parch', 'Fare', 'Ticket' ,'Cabin', 'Embarked', 'Name', 'Sex')\n",
    "train = train.drop(*cols)\n",
    "\n",
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: float (nullable = true)\n",
      " |-- Survived: float (nullable = true)\n",
      " |-- Pclass: float (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- Gender: float (nullable = true)\n",
      "\n",
      "+-----------+--------+------+----+------+--------------+\n",
      "|PassengerId|Survived|Pclass| Age|Gender|      features|\n",
      "+-----------+--------+------+----+------+--------------+\n",
      "|        1.0|     0.0|   3.0|22.0|   1.0|[3.0,22.0,1.0]|\n",
      "|        2.0|     1.0|   1.0|38.0|   2.0|[1.0,38.0,2.0]|\n",
      "|        3.0|     1.0|   3.0|26.0|   2.0|[3.0,26.0,2.0]|\n",
      "+-----------+--------+------+----+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "train = train.withColumn(\"PassengerId\", train[\"PassengerId\"].cast('float'))\n",
    "train = train.withColumn(\"Survived\" ,train[\"Survived\"].cast('float'))\n",
    "train = train.withColumn(\"Pclass\" ,train[\"Pclass\"].cast('float'))\n",
    "train = train.withColumn(\"Age\" ,train[\"Age\"].cast(('float')))\n",
    "train = train.withColumn(\"Gender\" ,train[\"Gender\"].cast(('float')))\n",
    "train.printSchema()\n",
    "features = ['Pclass','Age', 'Gender']\n",
    "va = VectorAssembler(inputCols = features, outputCol='features')\n",
    "va_df = va.transform(train)\n",
    "va_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_, test) = va_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_1a82c1af751b, depth=5, numNodes=23, numClasses=2, numFeatures=3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "dtc = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "\n",
    "#indexer = StringIndexer().setInputCol(\"Survived\").setOutputCol(\"label_idx\").fit(train_)\n",
    "\n",
    "prediction = dtc.fit(train_) #dtc.setLabelCol(\"label_idx\").fit(dtc)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+----+------+--------------+-------------+--------------------+----------+\n",
      "|PassengerId|label|Pclass| Age|Gender|      features|rawPrediction|         probability|prediction|\n",
      "+-----------+-----+------+----+------+--------------+-------------+--------------------+----------+\n",
      "|       11.0|  1.0|   3.0| 4.0|   2.0| [3.0,4.0,2.0]|    [4.0,9.0]|[0.30769230769230...|       1.0|\n",
      "|       12.0|  1.0|   1.0|58.0|   2.0|[1.0,58.0,2.0]|  [8.0,122.0]|[0.06153846153846...|       1.0|\n",
      "|       17.0|  0.0|   3.0| 2.0|   1.0| [3.0,2.0,1.0]|    [2.0,2.0]|           [0.5,0.5]|       0.0|\n",
      "+-----------+-----+------+----+------+--------------+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = prediction.transform(test)\n",
    "pred = pred.withColumnRenamed('Survived', 'label')\n",
    "pred.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  85.86322536304587\n",
      "Confusion Matrix:\n",
      "[[82  6]\n",
      " [13 36]]\n"
     ]
    }
   ],
   "source": [
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "acc = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Prediction Accuracy: \", acc*100)\n",
    "\n",
    "y_pred=pred.select(\"prediction\").collect()\n",
    "y_orig=pred.select(\"label\").collect()\n",
    "\n",
    "confusion_M = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.93      0.90        88\n",
      "         1.0       0.86      0.73      0.79        49\n",
      "\n",
      "    accuracy                           0.86       137\n",
      "   macro avg       0.86      0.83      0.84       137\n",
      "weighted avg       0.86      0.86      0.86       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_orig, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dtreeviz import trees\n",
    "#trees.dtreeviz(prediction, fancy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import graphviz\n",
    "#print(graphviz.__version__)\n",
    "\n",
    "#graphviz_tree = tree.export_graphviz(iowa_model, out_file=None, feature_names=features, filled=True)\n",
    "#graphviz.Source(graphviz_tree, format=\"png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+------+\n",
      "|PassengerId|Survived|Pclass| Age|Gender|\n",
      "+-----------+--------+------+----+------+\n",
      "|        1.0|     0.0|   3.0|22.0|   1.0|\n",
      "+-----------+--------+------+----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: PassengerId, Survived, Pclass, Age, Gender",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15288/2538899473.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdot_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_characters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test-spark.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benpr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\Users\\benpr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benpr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benpr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benpr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benpr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: PassengerId, Survived, Pclass, Age, Gender"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "feature_cols = ['PassengerId', 'Survived','Pclass', 'Age', 'Gender']\n",
    "train.show(1)\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dtc.fit(train), out_file=dot_data, filled=True, rounded=True, special_characters=True, feature_names = feature_cols, class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('test-spark.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f4e994e616bc70104392ad43172e29317900d3f313f6865515deae64a667d82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
