{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.sql.functions import when\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf()\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"test\").config(conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---+------+\n",
      "|PassengerId|Survived|Pclass|Age|Gender|\n",
      "+-----------+--------+------+---+------+\n",
      "|          1|       0|     3| 22|     1|\n",
      "|          2|       1|     1| 38|     2|\n",
      "|          3|       1|     3| 26|     2|\n",
      "|          4|       1|     1| 35|     2|\n",
      "|          5|       0|     3| 35|     1|\n",
      "|          7|       0|     1| 54|     1|\n",
      "|          8|       0|     3|  2|     1|\n",
      "|          9|       1|     3| 27|     2|\n",
      "|         10|       1|     2| 14|     2|\n",
      "|         11|       1|     3|  4|     2|\n",
      "+-----------+--------+------+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing the train\n",
    "train = spark.read.option(\"header\", \"true\").csv(r\"data\\train.csv\")\n",
    "# filtre et supprime les donn√©es \"NA\"\n",
    "train = train.filter(train.Age != \"NA\")\n",
    "train = train.withColumn(\"Gender\", when(train.Sex == \"male\",\"1\").when(train.Sex == \"female\",\"2\"))\n",
    "# Supprime les colonnes inutiles\n",
    "cols = ('SibSp', 'Parch', 'Fare', 'Ticket' ,'Cabin', 'Embarked', 'Name', 'Sex')\n",
    "train = train.drop(*cols)\n",
    "\n",
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: float (nullable = true)\n",
      " |-- Survived: float (nullable = true)\n",
      " |-- Pclass: float (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- Gender: float (nullable = true)\n",
      "\n",
      "+-----------+--------+------+----+------+--------------+\n",
      "|PassengerId|Survived|Pclass| Age|Gender|      features|\n",
      "+-----------+--------+------+----+------+--------------+\n",
      "|        1.0|     0.0|   3.0|22.0|   1.0|[3.0,22.0,1.0]|\n",
      "|        2.0|     1.0|   1.0|38.0|   2.0|[1.0,38.0,2.0]|\n",
      "|        3.0|     1.0|   3.0|26.0|   2.0|[3.0,26.0,2.0]|\n",
      "+-----------+--------+------+----+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "train = train.withColumn(\"PassengerId\", train[\"PassengerId\"].cast('float'))\n",
    "train = train.withColumn(\"Survived\" ,train[\"Survived\"].cast('float'))\n",
    "train = train.withColumn(\"Pclass\" ,train[\"Pclass\"].cast('float'))\n",
    "train = train.withColumn(\"Age\" ,train[\"Age\"].cast(('float')))\n",
    "train = train.withColumn(\"Gender\" ,train[\"Gender\"].cast(('float')))\n",
    "train.printSchema()\n",
    "features = ['Pclass','Age', 'Gender']\n",
    "va = VectorAssembler(inputCols = features, outputCol='features')\n",
    "va_df = va.transform(train)\n",
    "va_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_, test) = va_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a9841c9469ba, depth=5, numNodes=37, numClasses=2, numFeatures=3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "dtc = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "\n",
    "#indexer = StringIndexer().setInputCol(\"Survived\").setOutputCol(\"label_idx\").fit(train_)\n",
    "\n",
    "prediction = dtc.fit(train_) #dtc.setLabelCol(\"label_idx\").fit(dtc)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+----+------+--------------+-------------+--------------------+----------+\n",
      "|PassengerId|label|Pclass| Age|Gender|      features|rawPrediction|         probability|prediction|\n",
      "+-----------+-----+------+----+------+--------------+-------------+--------------------+----------+\n",
      "|        2.0|  1.0|   1.0|38.0|   2.0|[1.0,38.0,2.0]|   [3.0,70.0]|[0.04109589041095...|       1.0|\n",
      "|        9.0|  1.0|   3.0|27.0|   2.0|[3.0,27.0,2.0]|  [29.0,22.0]|[0.56862745098039...|       0.0|\n",
      "|       14.0|  0.0|   3.0|39.0|   1.0|[3.0,39.0,1.0]| [233.0,31.0]|[0.88257575757575...|       0.0|\n",
      "+-----------+-----+------+----+------+--------------+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = prediction.transform(test)\n",
    "pred = pred.withColumnRenamed('Survived', 'label')\n",
    "pred.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  77.35042735042735\n",
      "Confusion Matrix:\n",
      "[[85  8]\n",
      " [26 37]]\n"
     ]
    }
   ],
   "source": [
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "acc = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Prediction Accuracy: \", acc*100)\n",
    "\n",
    "y_pred=pred.select(\"prediction\").collect()\n",
    "y_orig=pred.select(\"label\").collect()\n",
    "\n",
    "confusion_M = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f4e994e616bc70104392ad43172e29317900d3f313f6865515deae64a667d82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
